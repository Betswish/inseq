{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "sample_text = \"Do know where the bus stop is?\"\n",
    "ids = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "gen = model.generate(**ids, return_dict_in_generate=True, output_scores=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(gen.sequences[0])\n",
    "gen_sentence = tokenizer.batch_decode(gen.sequences, skip_special_tokens=False)[0]\n",
    "print(\"Generated ids:\", gen.sequences[0].tolist())\n",
    "print(\"Corresponding tokens:\", tokens)\n",
    "print(\"Decoded sentence:\", gen_sentence,'\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated ids: [5211, 760, 810, 262, 1323, 2245, 318, 30, 198, 198, 464, 1323, 2245, 318, 5140, 287, 262, 3504, 286, 262]\n",
      "Corresponding tokens: ['Do', 'Ġknow', 'Ġwhere', 'Ġthe', 'Ġbus', 'Ġstop', 'Ġis', '?', 'Ċ', 'Ċ', 'The', 'Ġbus', 'Ġstop', 'Ġis', 'Ġlocated', 'Ġin', 'Ġthe', 'Ġmiddle', 'Ġof', 'Ġthe']\n",
      "Decoded sentence: Do know where the bus stop is?\n",
      "\n",
      "The bus stop is located in the middle of the \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = f'Helsinki-NLP/opus-mt-en-it'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, *{}, **{})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sample_text = \"Do you happen to know where the bus stop is?\"\n",
    "\n",
    "# Translating to Italian\n",
    "ids = tokenizer(sample_text,return_tensors=\"pt\")\n",
    "embeds = model.model.encoder.embed_tokens(ids['input_ids'])\n",
    "print(embeds.shape)\n",
    "\n",
    "#gen = model.generate(**ids, return_dict_in_generate=True, **{})\n",
    "#tokens = tokenizer.convert_ids_to_tokens(gen.sequences[0])\n",
    "#gen_sentence = tokenizer.batch_decode(gen.sequences, skip_special_tokens=False)[0]\n",
    "#print(\"Generated ids:\", gen.sequences[0].tolist())\n",
    "#print(\"Corresponding tokens:\", tokens)\n",
    "#print(\"Decoded sentence:\", gen_sentence,'\\n')\n",
    "#\n",
    "## Tokenizing the decoded sentence\n",
    "#with tokenizer.as_target_tokenizer():\n",
    "#    generated_tokens = tokenizer.tokenize(gen_sentence)\n",
    "#    out_ids = tokenizer.encode(gen_sentence)\n",
    "#print(generated_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 12, 512])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "\n",
    "#ids = torch.tensor([[[x * y for _ in range(513)] for x in range(1,14)] for y in range(1,11)])\n",
    "ids = torch.tensor([[x * y for x in range(1,14)] for y in range(1,11)])\n",
    "print(ids.shape)\n",
    "tgt_mask = torch.randint(0, 2, (10, 1))\n",
    "#tgt_mask = tgt_mask.unsqueeze(-1)\n",
    "print(tgt_mask.shape, tgt_mask)\n",
    "masked_ids = ids.masked_select(tgt_mask.bool()).reshape(-1, *ids.shape[1:])\n",
    "#print(masked_ids[:, :, 1])\n",
    "out = torch.rand(masked_ids.shape)\n",
    "index = tgt_mask.squeeze().nonzero().expand_as(masked_ids)\n",
    "#torch.zeros_like(ids, dtype=torch.float).scatter(0, index, out)\n",
    "source = torch.ones_like(ids, dtype=torch.float) * float(\"nan\")\n",
    "source.scatter(0, index, out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 13])\n",
      "torch.Size([10, 1]) tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan],\n",
       "        [0.6806, 0.2919, 0.7566, 0.9980, 0.9290, 0.8333, 0.1369, 0.1386, 0.1752,\n",
       "         0.2415, 0.3117, 0.8380, 0.7610],\n",
       "        [0.5897, 0.0402, 0.7865, 0.2741, 0.3121, 0.5003, 0.4818, 0.6672, 0.3434,\n",
       "         0.6939, 0.5428, 0.2278, 0.3984],\n",
       "        [0.7740, 0.6370, 0.4955, 0.5910, 0.5858, 0.1119, 0.4109, 0.7567, 0.8387,\n",
       "         0.9883, 0.2711, 0.2715, 0.3441],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan],\n",
       "        [0.2661, 0.5477, 0.6676, 0.6392, 0.2445, 0.3675, 0.0102, 0.6899, 0.2247,\n",
       "         0.7238, 0.8745, 0.5218, 0.5372],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from inspect import getfullargspec, signature\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "signature(model.generate).parameters.keys()\n",
    "\n",
    "model.get_input_embeddings() == model.get_encoder().get_input_embeddings()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotImplementedError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8239/3623748465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/amseq-xFN374Fl-py3.8/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mget_input_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "sentences = [\"My name is Giorgio\", \"I am a man\"]\n",
    "ids = tokenizer(sentences,add_special_tokens=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "gen = model.generate(**ids, return_dict_in_generate=True)\n",
    "print(gen.sequences)\n",
    "print(tokenizer.batch_decode(gen.sequences, skip_special_tokens=False))\n",
    "\n",
    "sentences_it = [\"Mi chiamo Giorgio.\", \"Sono un uomo.\"]\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    ids = tokenizer(sentences_it,add_special_tokens=True, return_tensors=\"pt\", truncation=True, padding=True).input_ids\n",
    "ids"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[80034,   253,  4429,    31, 15355,     2,     0],\n",
      "        [80034,   460,    23,  1732,     2,     0, 80034]])\n",
      "['<pad> Mi chiamo Giorgio.', '<pad> Sono un uomo.<pad>']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  253,  4429,    31, 15355,     2,     0],\n",
       "        [  460,    23,  1732,     2,     0, 80034]])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "ids.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids = tokenizer(\n",
    "    [sample_text],\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.max_len_single_sentence,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(ids)\n",
    "\n",
    "gen = model.generate(\n",
    "    **ids,\n",
    "    output_scores=True, return_dict_in_generate=True, use_cache=False\n",
    ")\n",
    "print(gen.sequences)\n",
    "tokenizer.batch_decode(gen.sequences, skip_special_tokens=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[ 629,   41, 4930,   12,  253,  299,    4, 4843, 2441,   19,   31,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[58100,  3613,    42, 16716,     2,   563,    11, 34153,    29,    31,\n",
      "             0]])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['Wissen Sie zufällig, wo die Bushaltestelle ist?']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from abc import ABC\n",
    "\n",
    "class A(ABC):\n",
    "    x = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"A\")\n",
    "    \n",
    "    @classmethod\n",
    "    def subclasses(cls, sup=None, hello=None):\n",
    "        registry = set()\n",
    "        for subclass in cls.__subclasses__():\n",
    "            if subclass not in registry:\n",
    "                registry.add(subclass)\n",
    "                registry |= subclass.subclasses()\n",
    "        return registry\n",
    "\n",
    "class B(A):\n",
    "    x = 3\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"B\")\n",
    "    \n",
    "    def test(self):\n",
    "        return 7\n",
    "\n",
    "class C(A):\n",
    "    x = 5\n",
    "    \n",
    "    def test(self):\n",
    "        return 9\n",
    "\n",
    "class D(B):\n",
    "    x = 7\n",
    "    pass\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class E(D):\n",
    "    a: list\n",
    "    b: str\n",
    "    c: list\n",
    "\n",
    "    def __getitem__(self, s):\n",
    "        return E(self.a[s], self.b[s], self.c[s])\n",
    "\n",
    "class F(B,C):\n",
    "    x = 11\n",
    "    \n",
    "    def test(self):\n",
    "        return super().test()\n",
    "\n",
    "#x = E([1,2,3,4,5], 'aeiou', [['hi'], ['hello'], ['hey'], ['yo'], ['hey']])\n",
    "#x = x[::-1]\n",
    "#x.a, x.b, x.c\n",
    "\n",
    "x = F()\n",
    "x.test()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A\n",
      "B\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "from mymarian import MyMarianMTModel\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianMTModel, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "#model_name = f'Helsinki-NLP/opus-mt-en-it'\n",
    "#model_name = 't5-small'\n",
    "model_name = f'Helsinki-NLP/opus-mt-en-de'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "#model.config.num_beams = 1\n",
    "\n",
    "#sample_text = \"translate English to German: My name is Sarah and I live in London\"\n",
    "sample_text = [\"Do you happen to know where the bus stop is?\", \"My name is Giorgio\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "type(model)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "transformers.models.marian.modeling_marian.MarianMTModel"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "batch = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "input_ids, attention_mask = batch.input_ids, batch.attention_mask\n",
    "gen = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    output_scores=True, return_dict_in_generate=True, use_cache=False\n",
    ")\n",
    "tokenizer.batch_decode(gen.sequences, skip_special_tokens=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<pad> Wissen Sie zufällig, wo die Bushaltestelle ist?']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "ids = tokenizer(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.max_len_single_sentence,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(ids)\n",
    "\n",
    "gen = model.generate(\n",
    "    **ids,\n",
    "    output_scores=True, return_dict_in_generate=True, use_cache=False\n",
    ")\n",
    "print(gen.sequences)\n",
    "tokenizer.batch_decode(gen.sequences, skip_special_tokens=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  629,    41,  4930,    12,   253,   299,     4,  4843,  2441,    19,\n",
      "            31,     0],\n",
      "        [  587,   671,    19, 45855,     0, 58100, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[58100,  3613,    42, 16716,     2,   563,    11, 34153,    29,    31,\n",
      "             0],\n",
      "        [58100,  1701,  1282,    29,  3649,   316,  4962,     3,     0, 58100,\n",
      "         58100]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Wissen Sie zufällig, wo die Bushaltestelle ist?', 'Mein Name ist Giorgio.']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "tokenizer(['Wissen Sie zufällig, wo die Bushaltestelle ist?'], return_tensors=\"pt\").input_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1699,  4534,    42,    24,   351,  6454,  4679,     2,   563,    11,\n",
       "         15573,   213,  3180,  2831,    19,    46,    31,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.tokenize('Wissen Sie zufällig, wo die Bushaltestelle ist?').input_ids)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'input_ids'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8575/1120345187.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wissen Sie zufällig, wo die Bushaltestelle ist?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'input_ids'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "mask = ids[\"input_ids\"]\n",
    "mask.ne(model.config.eos_token_id).long() * model.config.pad_token_id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034,\n",
       "         80034,     0],\n",
       "        [80034, 80034, 80034, 80034, 80034, 80034,     0, 80034, 80034, 80034,\n",
       "         80034, 80034]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "input_ids = tokenizer.encode(sample_text, add_special_tokens=True, truncation=True, max_length=tokenizer.max_len_single_sentence)\n",
    "input_ids = torch.tensor([input_ids], device=device, dtype=torch.long)\n",
    "attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "input_ids, attention_mask"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[  822,    10,   363,    19,  3833, 11516,     9,     3,    58,  2625,\n",
       "             10,    86,  7005,     7,    15,  9735,  1863,     6,  3833, 11516,\n",
       "              9,    19,     3,     9, 25941,     6, 10933,  6358,  1069,    16,\n",
       "            282,  6390,     6,     3, 16718,   147,    57,     8,  8581,  9899,\n",
       "             77,     5,     3,     1,     1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoder = model.get_encoder()\n",
    "inputs_embeds = encoder.embed_tokens(ids[\"input_ids\"]) * encoder.embed_scale\n",
    "print(inputs_embeds.shape)\n",
    "decoder_ids = torch.ones((ids[\"input_ids\"].shape[0], 1), dtype=torch.long) * model.config.decoder_start_token_id\n",
    "decoder = model.get_decoder()\n",
    "decoder_embeds = decoder.embed_tokens(decoder_ids) * decoder.embed_scale\n",
    "print(decoder_embeds.shape)\n",
    "out = model(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    decoder_inputs_embeds=decoder_embeds,\n",
    "    attention_mask=ids[\"attention_mask\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 12, 512])\n",
      "torch.Size([2, 1, 512])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "argmax_idxs = out.logits.squeeze(1).argmax(1)\n",
    "max_scores = out.logits.squeeze(1).max(1)\n",
    "print(argmax_idxs, max_scores)\n",
    "out.logits.squeeze(1).index_select(dim=1, index=argmax_idxs).diag().unsqueeze(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([494, 760]) torch.return_types.max(\n",
      "values=tensor([ 9.0862, 10.4151], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([494, 760]))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 9.0862],\n",
       "        [10.4151]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def predict(input_ids, attention_mask, decoder_input_ids=None):\n",
    "    global model\n",
    "    if decoder_input_ids is None:\n",
    "        decoder_input_ids = torch.ones((1, 1), dtype=torch.long, device=device) * model.config.decoder_start_token_id\n",
    "    outputs = model(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "    )\n",
    "    return outputs.logits[:, -1, :]\n",
    "\n",
    "def forward_func(input_ids, attention_mask, decoder_input_ids=None, verbose=False, output_token=False):\n",
    "    global tokenizer\n",
    "    logits = predict(input_ids, attention_mask, decoder_input_ids)\n",
    "    next_token = torch.argmax(logits, dim=-1)\n",
    "    score = logits.max(-1).values\n",
    "    if verbose:\n",
    "        print(\"Generated token:\", tokenizer.decode(next_token), f\"({int(next_token[0])}) \" \"with score\", float(score[0]))\n",
    "    if output_token:\n",
    "        return next_token\n",
    "    return score\n",
    "\n",
    "new_tok = None\n",
    "decoder_input_ids = torch.ones((1, 1), dtype=torch.long, device=device) * model.config.decoder_start_token_id\n",
    "while new_tok is None or new_tok[0] != model.config.eos_token_id:\n",
    "    if new_tok is not None:\n",
    "        decoder_input_ids = torch.cat((decoder_input_ids, new_tok[:, None]), dim=-1)\n",
    "    new_tok = forward_func(input_ids, attention_mask, decoder_input_ids=decoder_input_ids, verbose=True, output_token=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated token:  (3) with score -4.232519149780273\n",
      "Generated token: a (9) with score -13.264476776123047\n",
      "Generated token: majestic (25941) with score -4.463294982910156\n",
      "Generated token: , (6) with score -11.984636306762695\n",
      "Generated token: enormous (10933) with score -3.123307704925537\n",
      "Generated token: hall (6358) with score -9.972796440124512\n",
      "Generated token: located (1069) with score -5.084230899810791\n",
      "Generated token: in (16) with score -11.392878532409668\n",
      "Generated token: As (282) with score -9.637094497680664\n",
      "Generated token: gard (6390) with score -7.734871864318848\n",
      "Generated token: , (6) with score -3.8380584716796875\n",
      "Generated token:  (3) with score -6.948188304901123\n",
      "Generated token: ruled (16718) with score -7.179008483886719\n",
      "Generated token: over (147) with score -11.284448623657227\n",
      "Generated token: by (57) with score -11.73874282836914\n",
      "Generated token: the (8) with score -13.138486862182617\n",
      "Generated token: god (8581) with score -8.506855010986328\n",
      "Generated token: Od (9899) with score -7.856850624084473\n",
      "Generated token: in (77) with score -10.476737976074219\n",
      "Generated token: </s> (1) with score -9.324771881103516\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "lig = LayerIntegratedGradients(forward_func, model.model.encoder.embed_tokens)\n",
    "\n",
    "ref_input_ids = torch.tensor([[model.config.pad_token_id] * (input_ids.shape[1] - 1) + [model.config.eos_token_id]])\n",
    "decoder_input_ids = torch.ones((1, 1), dtype=torch.long, device=device) * model.config.decoder_start_token_id\n",
    "print(input_ids, ref_input_ids, decoder_input_ids)\n",
    "\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=ref_input_ids,\n",
    "    additional_forward_args=(attention_mask, decoder_input_ids, False, False),\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "delta, attributions"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 295,   29, 3710,   11,  172,  328,    4, 2040, 1770,   24,   19,    0]]) tensor([[80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034, 80034,\n",
      "         80034,     0]]) tensor([[80034]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([-2.2255], dtype=torch.float64),\n",
       " tensor([[[0., 0., 0., 0., 0., -0., -0., -0., 0., -0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., -0., -0., -0.,\n",
       "           0., -0., -0., 0., -0., -0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., 0., -0., -0., -0., -0.,\n",
       "           0., 0., -0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., 0., -0., 0., 0., -0., 0., -0., 0., -0., 0.,\n",
       "           0., -0., 0., -0., -0., -0., -0., 0., 0., -0., -0., 0., 0., -0., -0., 0., 0., -0., 0., 0., -0., 0., -0.,\n",
       "           -0., 0., -0., -0., 0., -0., 0., -0., -0., 0., -0., -0., 0., -0., -0., 0., -0., 0., -0., 0., -0., 0., 0.,\n",
       "           0., -0., -0., -0., -0., 0., -0., 0., 0., -0., 0., 0., 0., -0., 0., 0., 0., -0., -0., 0., -0., -0., -0.,\n",
       "           0., -0., 0., -0., -0., 0., -0., 0., -0., -0., -0., 0., -0., 0., -0., -0., -0., -0., -0., 0., 0., -0., -0.,\n",
       "           -0., -0., 0., 0., -0., -0., 0., -0., 0., -0., -0., 0., 0., 0., -0., -0., 0., 0., -0., -0., -0., -0., -0.,\n",
       "           0., 0., -0., -0., 0., 0., 0., 0., -0., 0., 0., -0., 0., -0., -0., 0., -0., 0., 0., 0., -0., -0., -0.,\n",
       "           0., -0., -0., 0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., -0.,\n",
       "           0., 0., -0., 0., 0., -0., 0., 0., -0., -0., 0., 0., -0., -0., 0., 0., 0., 0., 0., 0., 0., -0., 0.,\n",
       "           -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., -0., -0., -0., 0., -0., -0., 0., -0., -0., 0., 0., 0.,\n",
       "           -0., -0., 0., -0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., -0.,\n",
       "           0., 0., 0., 0., 0., -0., -0., -0., 0., 0., 0., -0., -0., -0., 0., -0., -0., 0., -0., 0., -0., -0., 0.,\n",
       "           -0., -0., -0., -0., 0., 0., -0., 0., -0., 0., -0., -0., 0., 0., 0., 0., 0., 0., 0., 0., -0., 0., 0.,\n",
       "           -0., -0., 0., -0., 0., -0., 0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., -0., -0., -0., -0.,\n",
       "           -0., 0., 0., 0., -0., -0., -0., -0., -0., 0., -0., -0., -0., 0., -0., -0., 0., -0., 0., -0., 0., -0., -0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0., 0., -0.,\n",
       "           0., -0., 0., 0., -0., 0., 0., 0., -0., 0., 0., -0., -0., 0., 0., -0., 0., 0., -0., -0., 0., 0., -0.,\n",
       "           0., -0., 0., -0., -0., 0., -0., -0., 0., 0., -0., 0., 0., -0., -0., 0., 0., 0., 0., -0., -0., 0., -0.,\n",
       "           0., -0., -0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0., -0., 0., 0., -0., 0., -0., 0., -0., 0., -0.,\n",
       "           0., -0., 0., 0., 0., 0., 0., 0., 0., -0., 0., 0., -0., -0., -0., 0., -0., -0., -0., -0., -0., -0., 0.,\n",
       "           -0., -0., 0., -0., -0., 0.]]], dtype=torch.float64))"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.tensor([model.config.decoder_start_token_id] + tokenizer.encode())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "decoder_input_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0,     3,     9, 25941,     6, 10933,  6358,  1069,    16,   282,\n",
       "          6390,     6,     3, 16718,   147,    57,     8,  8581,  9899,    77]])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from captum.attr import configure_interpretable_embedding_layer, IntegratedGradients, InterpretableEmbeddingBase\n",
    "\n",
    "if not isinstance(model.model.encoder.embed_tokens, InterpretableEmbeddingBase):\n",
    "    interpretable_emb = configure_interpretable_embedding_layer(model.model.encoder, \"embed_tokens\")\n",
    "if not isinstance(model.model.decoder.embed_tokens, InterpretableEmbeddingBase):\n",
    "    interpretable_emb_dec = configure_interpretable_embedding_layer(model.model.decoder, \"embed_tokens\")\n",
    "input_embeds = interpretable_emb.indices_to_embeddings(ids[\"input_ids\"])\n",
    "decoder_input_ids = torch.ones((2, 1), dtype=torch.long, device=device) * model.config.decoder_start_token_id\n",
    "decoder_input_embeds = interpretable_emb_dec.indices_to_embeddings(decoder_input_ids)\n",
    "print(input_embeds.shape, decoder_input_embeds.shape)\n",
    "\n",
    "def predict(inputs_embeds, attention_mask, decoder_inputs_embeds):\n",
    "    global model\n",
    "    outputs = model(\n",
    "        inputs_embeds=inputs_embeds, \n",
    "        attention_mask=attention_mask,\n",
    "        decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "    )\n",
    "    return outputs.logits[:, -1, :]\n",
    "\n",
    "def forward_func(inputs_embeds, attention_mask, decoder_inputs_embeds, verbose=False, output_token=False):\n",
    "    global tokenizer\n",
    "    logits = predict(inputs_embeds, attention_mask, decoder_inputs_embeds)\n",
    "    next_token = torch.argmax(logits, dim=-1)\n",
    "    score = logits.max(-1).values\n",
    "    if verbose:\n",
    "        print(\"Generated token:\", tokenizer.decode(next_token), f\"({int(next_token[0])}) \" \"with score\", float(score[0]))\n",
    "    if output_token:\n",
    "        return next_token\n",
    "    return score\n",
    "\n",
    "ig = IntegratedGradients(forward_func)\n",
    "attribution, delta = ig.attribute(input_embeds, additional_forward_args=(ids[\"attention_mask\"], decoder_input_embeds, False, False), return_convergence_delta=True)\n",
    "print(delta)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 12, 512]) torch.Size([2, 1, 512])\n",
      "tensor([2.7244e-06, 1.4541e-06], dtype=torch.float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "attribution.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(attribution.shape)\n",
    "attribution = attribution.sum(dim=-1).squeeze(0)\n",
    "attribution = attribution / torch.norm(attribution)\n",
    "attribution.shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 12, 512])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "attribution2 = attribution"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "attribution2 = attribution2 * 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "attribution2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 4.9009,  0.5630, -0.0612,  7.2077, -0.5418,  0.5328,  0.3939, -1.9363,\n",
       "        -3.0155, -2.0120,  2.3732,  0.6746], dtype=torch.float64,\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "attribution = attribution.unsqueeze(1)\n",
    "attribution2 = attribution2.unsqueeze(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "attribution.shape, attribution2.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([12, 1]), torch.Size([12, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "x = torch.cat((attribution, attribution2), dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "x[1,:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0563, 0.5630], dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\n",
    "from attr_model import AttributionModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "model_name = f'Helsinki-NLP/opus-mt-en-it'\n",
    "model = AttributionModel(model_name)\n",
    "sample_text = \"Hello world!\"\n",
    "out = model.attribute(sample_text, n_steps=50)\n",
    "model.visualize_attributions(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/gsarti/projects/ig_nmt/venv/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n",
      "INFO:attr_model:Original: Hello world!\n",
      "Generating: ▁Ciao,▁mondo!: : 7it [00:08,  1.21s/it]\n",
      "INFO:attr_model:Finished attributing generated string \"Ciao, mondo!\"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "out.deltas"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.28048738165746245,\n",
       " 0.5285938915833821,\n",
       " 0.21297562961929462,\n",
       " -1.3000355160509416,\n",
       " -1.002344384833507,\n",
       " 0.041601142474030656,\n",
       " -0.0637820084424518,\n",
       " 0.043083353398396085,\n",
       " 0.7208574100955614,\n",
       " 1.5340149417882891,\n",
       " 0.10151500315874351,\n",
       " 0.148479246777029,\n",
       " -0.032789700885819784,\n",
       " -0.05692243419200116,\n",
       " 0.1259051651032097,\n",
       " -1.2650261227160255,\n",
       " -0.006238259488731801]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('amseq-xFN374Fl-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "0f9cb8ba9305c109ca7b64a41ca66d14fe83cf77fc42860b5dba65eece3ab2aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
